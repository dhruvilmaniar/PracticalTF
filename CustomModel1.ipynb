{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CustomModel1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dhruvilmaniar/PracticalTF/blob/master/CustomModel1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MASXlhk_XDJ2",
        "colab_type": "text"
      },
      "source": [
        "# **Customizing tf.keras Sequential API**\n",
        "\n",
        "\n",
        "There is a scope of customization in the following areas of tf.keras API and Machine learning models:\n",
        "* Custom Model\n",
        "* Custom Loss Function\n",
        "* Custom Gradient Calculations / Metcrics\n",
        "* Custom Training Loop\n",
        "\n",
        "\n",
        "These points are discussed as follows:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eqw_WpfiXEM0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a590288e-f4cd-4076-b315-463d866a7258"
      },
      "source": [
        "%tensorflow_version 2.x\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 2.x selected.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "td_Y6-qkYeTV",
        "colab_type": "text"
      },
      "source": [
        "**Layers:**\n",
        "\n",
        "In the tf.keras.layers, layers are package. So, to create a layer, construct the object.\n",
        "\n",
        "Example layers are Dense, Conv2D, LSTM, BatchNormalization, Dropout etc."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KESw7lr4XEWE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "outputId": "e5513205-ac36-4f4c-a5a9-21739d9e2290"
      },
      "source": [
        "# Most of the layers pre-defined in tf.keras.layers takes the number of \n",
        "# output dimensions / channels as first argument.\n",
        "\n",
        "layer = tf.keras.layers.Dense(10)\n",
        "\n",
        "# Now, let's add something to the layer.\n",
        "\n",
        "layer(tf.zeros([10,5]))\n",
        "layer.kernel, layer.bias\n",
        "\n",
        "# As we have used Dense layer, there will be variables for weights and biases.\n",
        "layer.variables\n",
        "# We can access all the variables using layer.variables, or we can use \n",
        "# seperate methods to call the variables, as used above - layer.kernel,layer.bias."
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<tf.Variable 'dense_2/kernel:0' shape=(5, 10) dtype=float32, numpy=\n",
              " array([[-0.32117304,  0.22515261, -0.04932344,  0.5247615 , -0.5611075 ,\n",
              "          0.26869237,  0.31127644,  0.53428084, -0.6305981 , -0.47024637],\n",
              "        [ 0.25925165, -0.29755735, -0.30946788,  0.50860995,  0.39789575,\n",
              "         -0.5801243 ,  0.530909  , -0.5243641 ,  0.50558656, -0.5727969 ],\n",
              "        [ 0.611937  ,  0.04066783, -0.52270955,  0.25084978, -0.18456033,\n",
              "         -0.34910673, -0.4908241 , -0.3712563 , -0.56858516, -0.04765975],\n",
              "        [ 0.4944715 , -0.01739728,  0.44943827, -0.06578404, -0.6179091 ,\n",
              "          0.20468056,  0.00390184,  0.1117025 ,  0.0730257 , -0.557539  ],\n",
              "        [-0.49426073,  0.6175292 , -0.5147334 ,  0.13128686,  0.55248123,\n",
              "         -0.2263633 ,  0.63244075,  0.22972637, -0.13914627, -0.09317458]],\n",
              "       dtype=float32)>,\n",
              " <tf.Variable 'dense_2/bias:0' shape=(10,) dtype=float32, numpy=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cD0zD3T2a37S",
        "colab_type": "text"
      },
      "source": [
        "# Using Custom Layers:\n",
        "\n",
        "We can design custom layers by doing the following:\n",
        "\n",
        "* Extend the tf.keras.layers.Layer and Implement the following:\n",
        "  * `__init__` : Perform all Input Independent variable Initializations.\n",
        "  * `build` : Take the shape of Input tensors and perform the rest of Initializations.\n",
        "  * `call` : Where you do all the Forward Computations. (Linear combination & Non linear Activation.)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "klaaivhGXEZb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MyLayer(tf.keras.layers.Layer):\n",
        "\n",
        "  def __init__(self, num_outputs):\n",
        "    super(MyLayer, self).__init__()\n",
        "    self.outputs = num_outputs\n",
        "\n",
        "  def build(self, input_shape):\n",
        "    self.kernel = self.add_weight('kernel', shape = [int(input_shape[-1]), self.outputs])\n",
        "\n",
        "  def call(self, input):\n",
        "    return tf.matmul(input, self.kernel) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "48gY-V7fXEds",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "outputId": "67b51777-743a-4c78-a7e2-81fb19b7f309"
      },
      "source": [
        "# Let's call the layer, and initiate with number of outputs 10.\n",
        "layer = MyLayer(10)\n",
        "print(layer(tf.zeros([10,5])))\n",
        "print(layer.variables)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(\n",
            "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]], shape=(10, 10), dtype=float32)\n",
            "[<tf.Variable 'my_layer_3/kernel:0' shape=(5, 10) dtype=float32, numpy=\n",
            "array([[-0.09797525, -0.41580424, -0.06098711, -0.25298166, -0.05103385,\n",
            "        -0.5739522 ,  0.37411314,  0.18920869, -0.23845252,  0.4273755 ],\n",
            "       [-0.6214021 , -0.5337967 , -0.2350735 ,  0.4760211 , -0.24065194,\n",
            "        -0.5860123 ,  0.59674853, -0.2566024 , -0.26822916, -0.44599208],\n",
            "       [ 0.0068962 ,  0.20480573, -0.24739432, -0.3040443 ,  0.06602079,\n",
            "        -0.3320258 , -0.532031  , -0.11931115, -0.04683059,  0.00972092],\n",
            "       [-0.34104237,  0.27229005,  0.40946752, -0.5997822 , -0.28719   ,\n",
            "        -0.03497416, -0.1616211 , -0.2124016 , -0.34013796, -0.01817161],\n",
            "       [-0.07410294, -0.56458414,  0.2948659 , -0.5756861 ,  0.26216775,\n",
            "        -0.28179806,  0.48755473, -0.3749101 , -0.2638462 , -0.2589437 ]],\n",
            "      dtype=float32)>]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "StfDx2qBekaG",
        "colab_type": "text"
      },
      "source": [
        "# **Automatic Differentiation**\n",
        "\n",
        "Tensorflow provides tf.GradientTape API for automatic differentiation and computing gradients with respect to it's Inputs.\n",
        "\n",
        "* Tensorflow records all the operations executed inside the context of tf.GradientTape() onto a 'tape'.\n",
        "* Then it uses this tape to perform reverse mode differentaitaion, which is typically chain rule of differentiation. \n",
        "\n",
        "For example:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BMIs9oGjXEg6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "44bdbce9-4996-4898-f7c1-99533c2b4afc"
      },
      "source": [
        "x = tf.Variable(1.0)\n",
        "with tf.GradientTape() as tape:\n",
        "  y = x**2\n",
        "  z = y**2\n",
        "\n",
        "dz_dx, dz_dy = tape.gradient(z, [x,y])\n",
        "print(dz_dx.numpy())\n",
        "print(dz_dy.numpy())\n",
        "\n",
        "# Here, x = 1, y = x^2, z = y^2.\n",
        "# If we apply chain rule of derivative on dz/dx, you will get dz as 4.0."
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "4.0\n",
            "2.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ri807Di1hTNK",
        "colab_type": "text"
      },
      "source": [
        "By default, when we use tf.GradientTape.gradient() method, the resources used inside that context are released.\n",
        "\n",
        "Hence, we cannot use or cannot have more than one call of this method.\n",
        "\n",
        "This can be avoided by using persistant = True in the tape declaration."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oOrLAFEuXEj_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "9c71420e-a059-4eda-ba61-7d22ed21b8da"
      },
      "source": [
        "x = tf.constant(1.)\n",
        "with tf.GradientTape(persistent = True) as tape:\n",
        "  tape.watch(x) # We have to tell the tape that we are using x explictly, because x in not a tf variable.\n",
        "  y = x**2\n",
        "  z = y**2\n",
        "\n",
        "dz_dx = tape.gradient(z, x)\n",
        "dy_dx = tape.gradient(y, x)\n",
        "# Note that now we are able to use multiple calls to gradient method, which was not the case \n",
        "# in previouse cell.\n",
        "\n",
        "print(dz_dx)\n",
        "print(dy_dx)\n",
        "\n",
        "del tape \n",
        "# Never forget to release this, or this might manipulate further computations.\n",
        "# Also, Note that we get the same result as the previous cell."
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(4.0, shape=(), dtype=float32)\n",
            "tf.Tensor(2.0, shape=(), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GSiXp0QIjwYR",
        "colab_type": "text"
      },
      "source": [
        "**Finding Double Derivative / Higher order gradients**\n",
        "\n",
        "We can even use context manager to calculate higher order gradients:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jJqJ3PlOXEmX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "0b8fd89f-33d9-4bfe-9cf2-b3190d5ad53d"
      },
      "source": [
        "x = tf.Variable(1.)\n",
        "with tf.GradientTape() as tape1:\n",
        "  with tf.GradientTape() as tape2:\n",
        "    y = x * x * x\n",
        "\n",
        "  dy_dx = tape2.gradient(y,x)\n",
        "d2y_dx2 = tape1.gradient(dy_dx,x)\n",
        "\n",
        "print(dy_dx)\n",
        "print(d2y_dx2)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(3.0, shape=(), dtype=float32)\n",
            "tf.Tensor(6.0, shape=(), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}