{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GPUvsCPU_Acceleration.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dhruvilmaniar/PracticalTF/blob/master/GPUvsCPU_Acceleration.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t9CTnrgY_PTv",
        "colab_type": "text"
      },
      "source": [
        "# **GPU Acceleration**\n",
        "\n",
        "Tensorflow supports tensor operations on GPU also. Tensorflow automatically decides wheter to use CPU or GPU for tensor operation, and transfers tensors from GPU memory to CPU in such case.\n",
        "\n",
        "This automatic selection depends on various factors, with main being availablity of GPU.\n",
        "\n",
        "In tensorflow, we can test the availablity of GPU by the following command."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-E8QEumT_NhQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "69ddbbff-6ebc-4143-bcca-97012cd91dd4"
      },
      "source": [
        "%tensorflow_version 2.x\n",
        "# This command works on colab only, by the way. It helps if you have multiple\n",
        "# tensorflow versions installed in your environment.\n",
        "\n",
        "import tensorflow as tf\n",
        "import numpy as np"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 2.x selected.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ORbLXp7_Nni",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "814a422b-4bf9-43a0-bcaa-8a51a587b9ae"
      },
      "source": [
        "print(tf.test.is_gpu_available())\n",
        "# Note : In colab, you have to choose GPU as accelerator to make this True."
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PyrmWyCEA9E1",
        "colab_type": "text"
      },
      "source": [
        "**Check if the tensor is stored in GPU:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g7YdAC6z_Nso",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "e87772ec-348b-4bc5-c104-3b52bbcab047"
      },
      "source": [
        "# First, let's define a random tensor.\n",
        "\n",
        "x = tf.random.uniform([3,3])\n",
        "print(x)\n",
        "\n",
        "# To get complete path of where the tensor is stored, use the following command:\n",
        "# Here, complete path should is meant as device ID and GPU ID.\n",
        "print(x.device)\n",
        "\n",
        "# To check if the tensor is stored in GPU 0, use:\n",
        "print(x.device.endswith('GPU:0'))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(\n",
            "[[0.12202823 0.59920156 0.01163042]\n",
            " [0.79695415 0.89406943 0.6498815 ]\n",
            " [0.48698485 0.98201406 0.8783976 ]], shape=(3, 3), dtype=float32)\n",
            "/job:localhost/replica:0/task:0/device:GPU:0\n",
            "True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hqgvpEkAB5tL",
        "colab_type": "text"
      },
      "source": [
        "**How Path Names are Decided**\n",
        "\n",
        "Consider the following explanation for this:[link text](https:// [link text](https://))\n",
        "![Device:1:GPU:0](https://i.ibb.co/gTp23Rv/GPU-Names.jpg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7wMGacUuMIcG",
        "colab_type": "text"
      },
      "source": [
        "# **GPU vs CPU in Tensor Processing:**\n",
        "\n",
        "Here, we will calculate the time taken by GPU and the CPU to perform Tensor Operations."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1cobn27W_Nvd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import time\n",
        "\n",
        "def mat_mul(x):\n",
        "  start = time.time()\n",
        "\n",
        "  for _ in range(100):\n",
        "    tf.matmul(x,x)\n",
        "\n",
        "  result = time.time() - start\n",
        "\n",
        "  print(f'Time taken : {result*1000:0.2f} ms')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jMcjJAUZNPQj",
        "colab_type": "text"
      },
      "source": [
        "**Forcing Operation on CPU:**\n",
        "\n",
        "We can Force an Operation on CPU with the command:\n",
        "\n",
        "`with tf.device('CPU:0'):`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ulo4MqZ_Nyt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "387c9dc3-8bcc-44b7-8faf-671520101502"
      },
      "source": [
        "with tf.device('CPU:0'):\n",
        "  x = tf.random.uniform([1000,1000])\n",
        "  assert x.device.endswith('CPU:0')\n",
        "  print('ON CPU:')\n",
        "  mat_mul(x)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ON CPU:\n",
            "Time taken : 3718.00 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PnEljqywPDMX",
        "colab_type": "text"
      },
      "source": [
        "**Forcing Operations on CPU:**\n",
        "\n",
        "In the same manner, we can force the session to use GPU with following command:\n",
        "\n",
        "`with tf.device('CPU:0):`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vhKKsjZl_N1f",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "cc7fce1f-3cb8-4185-a6b0-03d6267a658f"
      },
      "source": [
        "with tf.device('GPU:0'):\n",
        "  x = tf.random.uniform([1000,1000])\n",
        "  assert x.device.endswith('GPU:0')\n",
        "  print('ON GPU:')\n",
        "  mat_mul(x)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ON GPU:\n",
            "Time taken : 15.62 ms\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}